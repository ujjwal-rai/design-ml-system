# -*- coding: utf-8 -*-
"""huggingface.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-S2xkVuz8TUfFqSfGVD2LERnsuwAgIkV
"""

!pip install huggingface_hub

import requests

API_URL = "https://api-inference.huggingface.co/models/cardiffnlp/twitter-roberta-base-sentiment-latest"
headers = {"Authorization": "Bearer hf"}
payload = {
    "inputs": "Today is a great day",
}

response = requests.post(API_URL, headers=headers, json=payload)
response.json()

# Use a pipeline as a high-level helper
from transformers import pipeline

pipe = pipeline("text2text-generation", model="google/flan-t5-base")

# Load model directly
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-base")

# Step 1: Prepare your input text
input_text = "Please answer to the following question. Who is going to be the next Ballon d'or?"
# Step 2: Tokenize the input
input_ids = tokenizer(input_text, return_tensors="pt").input_ids

# Step 3: Generate output
# Adjust max_length as needed
output_ids = model.generate(input_ids, max_length=50)

# Step 4: Decode the output
output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

print(output_text)

